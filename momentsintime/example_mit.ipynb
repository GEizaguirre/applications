{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lithops Moments in Time dataset example\n",
    "## Video/image prediction\n",
    "In this notebook we will process video clips from the MiT dataset at scale with Lithops\n",
    "by predicting the actions with a pretrained ResNet50 model and then counting how many\n",
    "occurrences of each category have been predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import builtins\n",
    "import torch.optim\n",
    "import torch.nn.parallel\n",
    "from torch import save, load\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils import extract_frames\n",
    "from models import load_model, load_transform, load_categories\n",
    "\n",
    "from lithops.multiprocessing import Pool, Queue\n",
    "from lithops.multiprocessing.util import get_uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backends\n",
    "The same program can be run in a local environtment with processes or executed by\n",
    "functions in the cloud. After we choose a backend, only a few file locations must\n",
    "be changed. In this example we will be using the cloud functions backend.\n",
    "\n",
    "We will be using a custom runtime for our functions which has torch, torchvision,\n",
    "ffmpeg and opencv-python modules already installed.\n",
    "We will store the pretrained weights in the cloud so that functions can access it.\n",
    "Then, after functions get the models weights they will start preprocessing input\n",
    "videos and inferring them one by one.\n",
    "  \n",
    "Later in this notebook, we will see a little improvement detail to this process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_EXEC = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = 'momentsintime/input_data'\n",
    "\n",
    "if LOCAL_EXEC:\n",
    "    import os\n",
    "    from builtins import open\n",
    "    pool_initargs = {\n",
    "        'compute_backend': 'localhost',\n",
    "        'storage_backend': 'localhost'\n",
    "        }\n",
    "    weights_location = '/dev/shm/model_weights'\n",
    "    INPUT_DATA_DIR = os.path.abspath(INPUT_DATA_DIR)\n",
    "\n",
    "else:\n",
    "    from lithops.cloud_proxy import os, open\n",
    "    pool_initargs = {\n",
    "        'compute_backend': 'ibm_cf',\n",
    "        'storage_backend': 'ibm_cos',\n",
    "        'runtime': 'dhak/pywren-runtime-pytorch:3.6',\n",
    "        'runtime_memory': 2048\n",
    "        }\n",
    "    weights_location = 'momentsintime/models/model_weights'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_locations = [os.path.join(INPUT_DATA_DIR, name) for name in os.listdir(INPUT_DATA_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have masked the `open` function and `os` module with a proxy\n",
    "to manage files from the cloud transparently.  \n",
    "We will use `builtins.open` from now on to explicitly access a local file as some accesses have to occur in the very same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained ResNet50 model weights and save them in a directory accessible by all functions (`weights_location`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'http://moments.csail.mit.edu/moments_models'\n",
    "WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar'\n",
    "\n",
    "if not os.access(WEIGHTS_FILE, os.R_OK):\n",
    "    os.system('wget ' + '/'.join([ROOT_URL, WEIGHTS_FILE]))\n",
    "\n",
    "with builtins.open(WEIGHTS_FILE, 'rb') as f_in:\n",
    "    weights = f_in.read()\n",
    "with open(weights_location, 'wb') as f_out:\n",
    "    f_out.write(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video prediction and reduce function code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SEGMENTS = 16\n",
    "\n",
    "# Get dataset categories\n",
    "categories = load_categories()\n",
    "\n",
    "# Load the video frame transform\n",
    "transform = load_transform()\n",
    "\n",
    "def predict_videos(queue, video_locations):\n",
    "    with open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        result = dict(key=video_loc)\n",
    "        result['prediction'] = (idx[0], round(float(probs[0]), 5))\n",
    "        result['iter_duration'] = time.time() - start\n",
    "        results.append(result)\n",
    "    queue.put(results)\n",
    "\n",
    "# Counts how many predictions of each category have been made\n",
    "def reduce(queue, n):\n",
    "    pred_x_categ = {}\n",
    "    for categ in categories:\n",
    "        pred_x_categ[categ] = 0\n",
    "\n",
    "    checkpoint = 0.2\n",
    "    res_count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        results = queue.get()\n",
    "        res_count += len(results)\n",
    "        for res in results:\n",
    "            idx, prob = res['prediction']\n",
    "            pred_x_categ[categories[idx]] += 1\n",
    "\n",
    "        # print progress\n",
    "        if i >= (N * checkpoint):\n",
    "            print('Processed {} results.'.format(res_count))\n",
    "            checkpoint += 0.2\n",
    "\n",
    "    return pred_x_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions\n",
    "Similar to the `multiprocessing` module API, we use a Pool to map the video keys\n",
    "across n workers (concurrency). However, we do not have to instantiate a Pool of\n",
    "n workers *specificly*, it is the map function that will invoke as many workers according\n",
    "to the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lithops v2.2.0 init for IBM Cloud Functions - Namespace: pol23btr%40gmail.com_dev - Region: eu_gb\n",
      "ExecutorID 6d3975/2 | JobID M000 - Selected Runtime: dhak/pywren-runtime-pytorch:3.6 - 2048MB \n",
      "ExecutorID 6d3975/2 | JobID M000 - Uploading function and data - Total: 65.4KiB\n",
      "ExecutorID 6d3975/2 | JobID M000 - Starting function invocation: predict_videos()  - Total: 100 activations\n",
      "Processed 21 results.\n",
      "Processed 41 results.\n",
      "Processed 64 results.\n",
      "Processed 84 results.\n",
      "\n",
      "Done.\n",
      "Videos processed: 110\n",
      "Total duration: 24.08 sec\n",
      "\n",
      "bicycling: 9\n",
      "juggling: 100\n",
      "mowing: 1\n"
     ]
    }
   ],
   "source": [
    "queue = Queue()\n",
    "pool = Pool(initargs=pool_initargs)\n",
    "\n",
    "# Slice data keys\n",
    "N = min(CONCURRENCY, len(video_locations))\n",
    "iterable = [(queue, video_locations[n::CONCURRENCY]) \n",
    "            for n in range(N)]\n",
    "\n",
    "# Map and reduce on the go\n",
    "start = time.time()\n",
    "pool.map_async(func=predict_videos, iterable=iterable)\n",
    "pred_x_categ = reduce(queue, N)\n",
    "end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "for categ, count in pred_x_categ.items():\n",
    "    if count != 0:\n",
    "        print('{}: {}'.format(categ, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "## Performance improvement\n",
    "Now, since we know every function will have to pull the model weights from\n",
    "the cloud storage, we can actually pack these weights with the runtime image\n",
    "and reduce the start-up cost substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_initargs['runtime'] = 'dhak/pywren-runtime-resnet'\n",
    "weights_location = '/momentsintime/model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_videos(queue, video_locations):\n",
    "    # force local file access on new weights_location\n",
    "    with builtins.open(weights_location, 'rb') as f:\n",
    "        model = load_model(f)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    local_video_loc = 'video_to_predict_{}.mp4'.format(get_uuid())\n",
    "\n",
    "    for video_loc in video_locations:\n",
    "        start = time.time()\n",
    "        with open(video_loc, 'rb') as f_in:\n",
    "            with builtins.open(local_video_loc, 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Obtain video frames\n",
    "        frames = extract_frames(local_video_loc, NUM_SEGMENTS)\n",
    "\n",
    "        # Prepare input tensor [num_frames, 3, 224, 224]\n",
    "        input_v = torch.stack([transform(frame) for frame in frames])\n",
    "\n",
    "        # Make video prediction\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_v)\n",
    "            h_x = F.softmax(logits, 1).mean(dim=0)\n",
    "            probs, idx = h_x.sort(0, True)\n",
    "\n",
    "        # Output the prediction\n",
    "        result = dict(key=video_loc)\n",
    "        result['prediction'] = (idx[0], round(float(probs[0]), 5))\n",
    "        result['iter_duration'] = time.time() - start\n",
    "        results.append(result)\n",
    "    queue.put(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lithops v2.2.0 init for IBM Cloud Functions - Namespace: ibm_com_dev - Region: eu_gb\n",
      "ExecutorID 6d3975/4 | JobID M000 - Selected Runtime: dhak/pywren-runtime-resnet - 2048MB \n",
      "ExecutorID 6d3975/4 | JobID M000 - Uploading function and data - Total: 65.4KiB\n",
      "ExecutorID 6d3975/4 | JobID M000 - Starting function invocation: predict_videos()  - Total: 100 activations\n",
      "Processed 21 results.\n",
      "Processed 41 results.\n",
      "Processed 66 results.\n",
      "Processed 86 results.\n",
      "\n",
      "Done.\n",
      "Videos processed: 110\n",
      "Total duration: 18.93 sec\n",
      "\n",
      "bicycling: 9\n",
      "juggling: 100\n",
      "mowing: 1\n"
     ]
    }
   ],
   "source": [
    "queue = Queue()\n",
    "pool = Pool(initargs=pool_initargs)\n",
    "\n",
    "# Slice data keys\n",
    "N = min(CONCURRENCY, len(video_locations))\n",
    "iterable = [(queue, video_locations[n::CONCURRENCY]) \n",
    "            for n in range(N)]\n",
    "\n",
    "# Map and reduce on the go\n",
    "start = time.time()\n",
    "r = pool.map_async(func=predict_videos, iterable=iterable)\n",
    "pred_x_categ = reduce(queue, N)\n",
    "end = time.time()\n",
    "    \n",
    "print('\\nDone.')\n",
    "print('Videos processed:', len(video_locations))\n",
    "print('Total duration:', round(end - start, 2), 'sec\\n')\n",
    "\n",
    "for categ, count in pred_x_categ.items():\n",
    "    if count != 0:\n",
    "        print('{}: {}'.format(categ, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(weights_location)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove(WEIGHTS_FILE)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfiles and build scripts for both runtimes can be found in the runtime/ folder.\n",
    "\n",
    "### Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models\n",
    "\n",
    "### Moments in Time article: http://moments.csail.mit.edu/#paper\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit325e2e176c1e4c56af6d2bec6f3f9965"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}